{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ej_interpretacion.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPYs1PzjRzrv"
      },
      "source": [
        "## ¿Qué interpretación tiene la cantidad $\\dfrac{\\vert\\vert \\theta^* {\\vert\\vert}^2}{\\gamma^2}$ en la prueba de la convergencia del perceptrón?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOwyjEhtSK4W"
      },
      "source": [
        "En la demostracion de la convergencia del algoritmo del percetron, la relación que existe entre la norma del vector ideal y el margen de separacion de los puntos, se basa a partir de la conclusion final donde :\n",
        "\n",
        "*   $\\cos(\\theta ^*, \\theta^k)  \\geq  \\frac{k  \\gamma }{ \\sqrt{k R^2} || \\theta ^*||}$\n",
        "*   $ (\\theta^*)^T\\theta^{(k)} \\geq k\\gamma$\n",
        "*   por lo que concluimos $|| \\theta^{(k)}||\\cdot|| \\theta^{*}\\geq k\\gamma$\n",
        "\n",
        "Por lo que despejando la norma del vector ideal tenemos $|| \\theta^{(k)}||^2 \\geq  \\dfrac{k^2\\gamma^2}{|| \\theta^{*}||^2}$, por lo que reemplazamos esa norma por $k R^2$(por lo ques pude ser acotada por está), por lo que simplificando $k$ tenemos   $k \\leq \\dfrac{|| \\theta^{*}||^2 R^2}{\\gamma^2}$.\n",
        "Entonces $R^2$ es la bola con la que estamos acotando nuestros datos,y será una constante mayor a $0$ para la desigualdad  $k \\leq \\dfrac{|| \\theta^{*}||^2}{\\gamma^2}$ entonces esta relacion cuenta como una cota superior de el numero maximo de iteraciones qur debe hacer el algoritmo para converger, dando información sobre su complejidad."
      ]
    }
  ]
}